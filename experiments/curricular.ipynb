{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up for curricular experiment\n",
    "\n",
    "This assumes you have already followed the instructions in `baselines/baseline_t5`, which will set up the baseline clue files\n",
    "\n",
    "### Datasets\n",
    "1. Download and unzip the xd cw crossword set from http://xd.saul.pw/xd-clues.zip.\n",
    "    - Save it as './data/original/xd/clues.tsv'\n",
    "2. Preprocess the dataset using this notebook\n",
    "3. The dataset will be saved to k_acw_export_dir (as a single train.json file)\n",
    "4. We will also produce the anagram dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No gsheets writer is configured\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../decrypt')\n",
    "sys.path.append('../')\n",
    "\n",
    "from collections import defaultdict\n",
    "from decrypt.scrape_parse.acw_load import get_clean_xd_clues\n",
    "import config\n",
    "from decrypt.common.util_data import clue_list_tuple_to_train_split_json\n",
    "from decrypt.common import validation_tools as vt\n",
    "\n",
    "k_xd_orig_tsv = config.DataDirs.OriginalData.k_xd_cw\n",
    "k_acw_export_dir = config.DataDirs.DataExport.xd_cw_json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defaults to strip periods, remove questions, remove abbrevs, remove fillin\n",
    "stc_map, all_clues = get_clean_xd_clues(k_xd_orig_tsv,\n",
    "                                        remove_if_not_in_dict=False,\n",
    "                                        do_filter_dupes=True)\n",
    "clue_list_tuple_to_train_split_json((all_clues,),\n",
    "                                    comment='ACW set; xd cw set, all',\n",
    "                                    export_dir=k_acw_export_dir,\n",
    "                                    overwrite=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading file into dict: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/generated/twl_dict.txt\n",
      "32188it [00:00, 321873.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a spellchecker\n",
      "This will fail if you have not downloaded or generated twl_dict.txt\n",
      "DEL called for spellchecker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178691it [00:00, 336759.43it/s]\n",
      "INFO:root:Done reading file: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/generated/twl_dict.txt\n",
      "INFO:root:Reading file into dict: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/original/us/US.dic\n",
      "118619it [00:00, 333901.88it/s]\n",
      "INFO:root:Done reading file: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/original/us/US.dic\n",
      "INFO:root:Done setting up spellchecker\n",
      "INFO:root:Adding to db /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/generated/anag_db with updateflag new\n",
      "190717it [03:11, 998.48it/s] \n",
      "1it [00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 103379, 2: 49359, 3: 10649, 4: 4179, 5: 683, 6: 106, 7: 20, 8: 3, 9: 1, 10: 1})\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118619it [01:05, 1799.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 118609, 'dupes': 80781})\n",
      "Done.\n",
      "DEL called for spellchecker\n"
     ]
    }
   ],
   "source": [
    "# produce anagram datasets\n",
    "# roughly 3 minutes to complete\n",
    "from decrypt.common import anagrammer\n",
    "anagrammer.gen_db_with_both_inputs(update_flag=\"new\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from decrypt.common.util_data import (\n",
    "    get_anags,\n",
    "    write_json_tuple\n",
    ")\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def make_anag_sets_json():\n",
    "    all_anags = get_anags(max_num_words=-1)\n",
    "    json_list = []\n",
    "    for idx, a_list in enumerate(all_anags):\n",
    "        json_list.append(dict(idx=idx,\n",
    "                              anag_list=a_list))\n",
    "    print(json_list[0])\n",
    "\n",
    "    # normally would be (idx, input, tgt)\n",
    "    output_tuple = [json_list,]\n",
    "\n",
    "    os.makedirs(config.DataDirs.DataExport.anag_dir)\n",
    "    write_json_tuple(output_tuple,\n",
    "                     comment=\"List of all anagram groupings\",\n",
    "                     export_dir=config.DataDirs.DataExport.anag_dir,\n",
    "                     overwrite=False)\n",
    "\n",
    "def make_anag_indic_list_json():\n",
    "    # make the indicator list\n",
    "    with open(config.DataDirs.OriginalData.k_deits_anagram_list, 'r') as f:\n",
    "        all_anag_indicators = f.readlines()\n",
    "\n",
    "    final_indic_list = []\n",
    "    for a in all_anag_indicators:\n",
    "        final_indic_list.append(a.replace('_', \" \").strip())\n",
    "    with open(config.DataDirs.DataExport.anag_indics, 'w') as f:\n",
    "        json.dump(final_indic_list,f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "make_anag_sets_json()\n",
    "make_anag_indic_list_json()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Curricular training\n",
    "1. At this point you should have a files at\n",
    " - `./data/clue_json/curricular/ACW/train.json`\n",
    " - `./data/clue_json/curricular/anagram/[train.json, anag_indics.json]`\n",
    "\n",
    "2. Running curricular training is the same as running main t5 vanilla train, except that we pass an extra multitask flag, which specifies the curriculum to use. See `seq2seq/multitask_config`. You should pass one of the names from  `multi_config` dict in that file\n",
    "\n",
    "For example, to train the naive split with the top performing curricular approach (i.e. the result in table 3 that is ACW + ACW-descramble)\n",
    "```python\n",
    "train_clues.py --default_train=base --name=naive_top_curricular --project=curricular --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random' --multitask=ACW__ACW_descramble\n",
    "```\n",
    "\n",
    "Note that the modifications on the dataset are done at the\n",
    "\n",
    "3. To produce Table 3 of the results\n",
    "    -  we don't need to do a model_eval run since the outputted predictions have 5 generations\n",
    "       (which is all we report for that table (for faster experimental iteration).\n",
    "    - we need to run `load_and_run_t5` on all outputs (column 1) and on the anagram subset (column 2)\n",
    "      See below for how we do this.\n",
    "\n",
    "4. For our top result in Table 2 (main resuls) we\n",
    "    1. scale up the curricular period (to 4 total epochs)\n",
    "```python\n",
    "train_clues.py --default_train=base --name=naive_top_curricular --project=curricular --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_random' --multitask=final_top_result_scaled_up\n",
    "```\n",
    "    2. eval with full 100 generations, as before:\n",
    "e.g., if epoch 10 is best (you'll need to set the run_name)\n",
    "This runs the eval set (change the run_name)\n",
    "```python\n",
    "train_clues.py --default_val=base --name=curricular_naive_top --project=curricular --data_dir='../data/clue_json/guardian/naive_random' --ckpt_path='./wandb/run_name/files/epoch_10.pth.tar\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from decrypt.scrape_parse.guardian_load import load_guardian_splits\n",
    "import config\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No gsheets writer is configured\n",
      "INFO:decrypt.scrape_parse.guardian_load:loading from /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/puzzles\n",
      "INFO:decrypt.scrape_parse.guardian_load:Using file glob at /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/puzzles/cryptic*.json\n",
      "INFO:decrypt.scrape_parse.guardian_load:Glob has size 5518\n",
      "INFO:decrypt.scrape_parse.guardian_load:Glob size matches the expected one from Decrypting paper\n",
      "100%|██████████| 5518/5518 [01:23<00:00, 66.35it/s]\n",
      "  5%|▍         | 6769/143991 [00:00<00:06, 19804.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"length punct: '\", 1),\n",
      " ('invalid: clue group', 7687),\n",
      " ('invalid: invalid start char (most are continuation clues)', 607),\n",
      " ('invalid: number in clue (commonly references another clue)', 7066),\n",
      " ('invalid: regexp', 75),\n",
      " ('invalid: soln length does not match specified lens (multi box soln)', 56),\n",
      " ('invalid: unrecognized char in clue (e.g. html)', 85),\n",
      " ('invalid: zero-len clue text after regexp', 15),\n",
      " ('length punct: ,', 24644),\n",
      " ('length punct: -', 4148),\n",
      " ('length punct: .', 8),\n",
      " ('length punct: /', 1),\n",
      " ('stat: parsed_puzzle', 5518),\n",
      " ('stat: total_clues', 143991),\n",
      " (1, 119956),\n",
      " (2, 20272),\n",
      " (3, 2957),\n",
      " (4, 686),\n",
      " (5, 112),\n",
      " (6, 8)]\n",
      "Total clues: len(puzz_list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143991/143991 [00:00<00:00, 228548.02it/s]\n",
      "100%|██████████| 55783/55783 [00:03<00:00, 15482.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 1611 exact dupes\n",
      "142380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:decrypt.scrape_parse.guardian_load:Counter({1: 118540, 2: 20105, 3: 2929, 4: 686, 5: 112, 6: 8})\n",
      "INFO:decrypt.scrape_parse.guardian_load:Clue list length matches Decrypting paper expected length\n",
      "INFO:decrypt.scrape_parse.guardian_load:Got splits of lenghts [85428, 28476, 28476]\n",
      "INFO:decrypt.scrape_parse.guardian_load:First three clues of train set:\n",
      "\t[GuardianClue(clue='Suffering to grasp edge of plant', lengths=[8], soln='agrimony', soln_with_spaces='agrimony', idx=85002, dataset=PosixPath('/Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/puzzles'), across_or_down='across', pos=(7, 4), unique_clue_id='cryptic_25415_11-across', type='cryptic', number=25415, id='crosswords/cryptic/25415', creator='Chifonie', orig_lengths='8', lengths_punctuation=set()), GuardianClue(clue='Honour Ben and Noel with new order', lengths=[7], soln='ennoble', soln_with_spaces='ennoble', idx=3432, dataset=PosixPath('/Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/puzzles'), across_or_down='down', pos=(7, 8), unique_clue_id='cryptic_24994_18-down', type='cryptic', number=24994, id='crosswords/cryptic/24994', creator='Rufus', orig_lengths='7', lengths_punctuation=set()), GuardianClue(clue='Bit the royal we love? Cheers!', lengths=[4], soln='iota', soln_with_spaces='iota', idx=25530, dataset=PosixPath('/Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/puzzles'), across_or_down='across', pos=(0, 10), unique_clue_id='cryptic_26632_21-across', type='cryptic', number=26632, id='crosswords/cryptic/26632', creator='Screw', orig_lengths='4', lengths_punctuation=set())]\n",
      "INFO:decrypt.scrape_parse.guardian_load:Verifying splits match Decrypting paper: Spot test clue 5111 has correct text\n",
      "100%|██████████| 142380/142380 [00:08<00:00, 17357.17it/s]\n"
     ]
    }
   ],
   "source": [
    "def make_label_set():\n",
    "    _, all_clues, (train, _, _) = load_guardian_splits(config.DataDirs.Guardian.json_folder, verify=True)\n",
    "    labels: Dict[str, Set[int]] = defaultdict(set)       # set of the indices for this type\n",
    "    any_label = set()\n",
    "    def add_to_labels(name, idx, verify=True):\n",
    "        if verify:\n",
    "            assert idx not in any_label\n",
    "        any_label.add(idx)\n",
    "        labels[name].add(idx)\n",
    "\n",
    "    class PunctStripper:\n",
    "        \"\"\"\n",
    "        use to strip punctuation from clues (since punct is not part of outputs)\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            self.table_spaces = str.maketrans('','',string.punctuation + \" \")   # map punct and space to ''\n",
    "            self.punct_to_space_table = str.maketrans(string.punctuation,' '*len(string.punctuation))   # map punct to space\n",
    "        def strip(self, s: str, strip_spaces=True):\n",
    "            \"\"\"\n",
    "            :param s:\n",
    "            :param strip_spaces: if true, will remove spaces; otherwise all punct will be substituted\n",
    "            with a space, which is important for generating anagram outputs\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if strip_spaces:\n",
    "                return s.translate(self.table_spaces)\n",
    "            else:\n",
    "                return s.translate(self.punct_to_space_table)\n",
    "    ps = PunctStripper()\n",
    "\n",
    "    # will find hiddens / reversals (which are either direct, or direct reverse)\n",
    "    # the anagrams that result potentially take single letters from the start or end of another word\n",
    "\n",
    "    for sc in tqdm(all_clues):\n",
    "        c = ps.strip(sc.clue).lower()\n",
    "        s = sorted(sc.soln.lower())\n",
    "        tgt_len = len(s)\n",
    "        for idx in range(0, len(c) - tgt_len + 1):\n",
    "            sub_part = c[idx:idx+tgt_len]\n",
    "            # hidden if directly occurs\n",
    "            if sub_part == sc.soln.lower():\n",
    "                add_to_labels('hidden', sc.idx)\n",
    "                break\n",
    "            # reverse if occurs backward\n",
    "            if sub_part == sc.soln.lower()[::-1]:\n",
    "                add_to_labels('reverse', sc.idx)\n",
    "                break\n",
    "            # direct anagram if occurs directly in clue once spaces and punct removed\n",
    "            if sorted(sub_part) == s:\n",
    "                add_to_labels('anag_direct', sc.idx)\n",
    "                break\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels = make_label_set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28476\n",
      "[('agg_filter_len_pre_truncate', 4.572166034555415),\n",
      " ('agg_filtered_few', 1.0),\n",
      " ('agg_generate_few', 1.0),\n",
      " ('agg_generate_none', 0.0),\n",
      " ('agg_in_filtered', 0.3338952100014047),\n",
      " ('agg_in_sample', 0.3338952100014047),\n",
      " ('agg_sample_len', 5.0),\n",
      " ('agg_sample_len_correct', 0.919244275881444),\n",
      " ('agg_sample_len_pre_truncate', 5.0),\n",
      " ('agg_sample_wordct_correct', 0.9760640539401602),\n",
      " ('agg_top_10_after_filter', 0.3338952100014047),\n",
      " ('agg_top_match', 0.20213513133867117),\n",
      " ('agg_top_match_len_correct', 0.9922039612305099),\n",
      " ('agg_top_match_none', 0.007796038769490097),\n",
      " ('agg_top_match_wordct_correct', 0.9868310155920775),\n",
      " ('agg_top_sample_result_len_correct', 0.9403708386009271),\n",
      " ('agg_top_sample_result_wordct_correct', 0.9830734653743504),\n",
      " ('filter_len_pre_truncate', 130197),\n",
      " ('filtered_few', 28476),\n",
      " ('generate_few', 28476),\n",
      " ('generate_none', 0),\n",
      " ('in_filtered', 9508),\n",
      " ('in_sample', 9508),\n",
      " ('sample_len', 142380),\n",
      " ('sample_len_correct', 130882),\n",
      " ('sample_len_pre_truncate', 142380),\n",
      " ('sample_wordct_correct', 138972),\n",
      " ('top_10_after_filter', 9508),\n",
      " ('top_match', 5756),\n",
      " ('top_match_len_correct', 28254),\n",
      " ('top_match_none', 222),\n",
      " ('top_match_wordct_correct', 28101),\n",
      " ('top_sample_result_len_correct', 26778),\n",
      " ('top_sample_result_wordct_correct', 27994),\n",
      " ('total', 28476)]\n"
     ]
    }
   ],
   "source": [
    "# note that this should be run directly on the top model output from curricular training\n",
    "# otherwise (eg. if 100 beams were used), the top 5 output\n",
    "# sequences would be expected to change\n",
    "# remember not to append .json\n",
    "\n",
    "# eval on the full output (5 beams / 5 sequences)\n",
    "# this is column 1 of table 3\n",
    "vt.load_and_run_t5('outputs/model_output.preds',\n",
    "                   # pre_truncate=5,        # should not be needed since we have only 5 outputs\n",
    "                   do_length_filter=True)\n",
    "\n",
    "# run on the anagram subset\n",
    "# this is column 2 of table 3\n",
    "vt.load_and_run_t5('outputs/model_output.preds',\n",
    "                   filter_fcn=vt.make_set_filter(labels, 'anag_direct'),\n",
    "                   # pre_truncate=5,\n",
    "                   do_length_filter=True)\n",
    "\n",
    "# we are looking at agg_top_match (which is after filter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}