{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../decrypt')\n",
    "sys.path.append('../')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Analysis section of paper\n",
    "\n",
    "## 6.1 Meta-linguistic properties\n",
    "Table 3b is produced from the statistics that will be outputted in baselines/baseline_t5 and\n",
    "baselines/baseline_knn.\n",
    "\n",
    "Nothing else is required\n",
    "\n",
    "## 6.2 Disjointness\n",
    "This section contains two pars:\n",
    "1. Subset analysis\n",
    "2. Analysis on distinct splits\n",
    "\n",
    "Producing the table rows:\n",
    "1. Row 1 in the Table 3b (naive, entire split) is the same as T5 (lenghts) in the main\n",
    " results table and is produced by running baselines/baseline_t5, i.e.\n",
    "1. Subset not in train - see below for setup\n",
    "1. Naive disjoint - run in the same way as baselines/baseline_t5, but on the naive_disjoint split, i.e.\n",
    "    - Train on the naive disjoint split:\n",
    "```python\n",
    "train_clues.py --default_train=base --name=baseline_naive_disjoint --project=baseline --wandb_dir='./wandb' --data_dir='../data/clue_json/guardian/naive_disjoint'\n",
    "```\n",
    "    - Eval to get 100 generations (on the best epoch checkpoint), with and without `--test` flag\n",
    "```python\n",
    "train_clues.py --default_val=base --name=baseline_naive_disjoint_val --project=baseline --data_dir='../data/clue_json/guardian/naive_disjoint' --ckpt_path='./wandb/run_name/files/epoch_10.pth.tar\n",
    "```\n",
    "    - run the eval script\n",
    "`vt.load_and_run_t5('decrypt/t5_outputs/baseline_naive_disjoint_val.preds')`\n",
    "1. word initial disjoint split - same as T5 (lengths) row in main results table (see baselines/baselines_t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No gsheets writer is configured\n"
     ]
    }
   ],
   "source": [
    "# eval on the subset not seen in train\n",
    "import config\n",
    "from scrape_parse import load_guardian_splits\n",
    "from decrypt.common import validation_tools as vt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a function that will filter to those input-output pairs with answers\n",
    "# not seen during training\n",
    "def make_filter_fn():\n",
    "    _, _, (train, _, _) = load_guardian_splits(config.DataDirs.Guardian.json_folder, verify=True)\n",
    "    s = set()\n",
    "    for c in train:\n",
    "        s.add(c.soln_with_spaces)\n",
    "\n",
    "    # return False to omit\n",
    "    def filter_fcn(mp: vt.ModelPrediction):\n",
    "        if mp.target in s:\n",
    "            return False\n",
    "        return True\n",
    "    return filter_fcn\n",
    "\n",
    "\n",
    "vt.load_and_run_t5('outputs/naive_baseline.preds',\n",
    "                   filter_fcn=make_filter_fn())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the following is not reported in the paper\n",
    "# prepare for set that does not overlap, fuzzily\n",
    "# ie also match on plurals\n",
    "def make_set_inclusion_filter_fcn_fuzz():\n",
    "    _, _, (train, _, _) = load_guardian_splits(config.DataDirs.Guardian.json_folder, verify=True)\n",
    "    s = set()\n",
    "\n",
    "    # fuzzily match plurals\n",
    "    for c in train:\n",
    "        soln = c.soln_with_spaces\n",
    "        if soln.endswith('es'):\n",
    "            s.add(soln[:-2])\n",
    "        if soln.endswith('s'):\n",
    "            s.add(soln[:-1])\n",
    "        s.add(soln + 'es')\n",
    "        s.add(soln + 's')\n",
    "        s.add(soln)\n",
    "\n",
    "    # return False to omit\n",
    "    def filter_fcn(mp: vt.ModelPrediction):\n",
    "        if mp.target in s or mp.target[:-1] in s or mp.target[:-2] in s:\n",
    "            return False\n",
    "        return True\n",
    "    return filter_fcn\n",
    "\n",
    "vt.load_and_run_t5('outputs/naive_baseline.preds',\n",
    "                   filter_fcn=make_set_inclusion_filter_fcn_fuzz())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Wordplay minimal task\n",
    "- dataset\n",
    "- two descramble tasks\n",
    "- direct copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading xd (ACW) set from /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/original/xd/clues.tsv\n",
      "INFO:root:Reading file into dict: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/generated/twl_dict.txt\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a spellchecker\n",
      "This will fail if you have not downloaded or generated twl_dict.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178691it [00:00, 266654.85it/s]\n",
      "INFO:root:Done reading file: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/generated/twl_dict.txt\n",
      "INFO:root:Reading file into dict: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/original/us/US.dic\n",
      "118619it [00:00, 205430.77it/s]\n",
      "INFO:root:Done reading file: /Users/jsrozner/MOUNT/scdt/cryptic_nlp/decrypt_root/data/original/us/US.dic\n",
      "INFO:root:Done setting up spellchecker\n",
      "989205it [01:39, 17980.76it/s]"
     ]
    }
   ],
   "source": [
    "# first prepare the descrambling dataset\n",
    "from decrypt.scrape_parse.acw_load import get_clean_xd_clues\n",
    "from sklearn.model_selection import train_test_split\n",
    "from decrypt.common.util_data import write_json_tuple\n",
    "import random\n",
    "\n",
    "k_xd_orig_tsv = config.DataDirs.OriginalData.k_xd_cw\n",
    "k_descramble_rand = config.DataDirs.DataExport.descramble_random\n",
    "k_descramble_disj = config.DataDirs.DataExport.descramble_word_init_disjoint\n",
    "\n",
    "# method will produce a version of the ACW dataset that is\n",
    "# - single words that appear in our dictionary\n",
    "# - without exact duplicates (but note that some answers will occur multiple times with different clues\n",
    "# - filtered to answer words with between 4 and 14 characters\n",
    "# - downsampled to 10%\n",
    "def make_descramble_json(seed=42):\n",
    "    stc_map, all_clues = get_clean_xd_clues(k_xd_orig_tsv,\n",
    "                                            remove_if_not_in_dict=True,\n",
    "                                            do_filter_dupes=True)\n",
    "\n",
    "    # further filter away anything < 4 chars\n",
    "    all_clues_len = [x for x in all_clues if len(x.soln) > 3 and len(x.soln) < 15]\n",
    "    print(len(all_clues_len))\n",
    "\n",
    "    # downsample to 10 percent\n",
    "    rng = random.Random(42)\n",
    "    all_clues_10per = rng.sample(all_clues_len, k=int(len(all_clues)*.1))\n",
    "    print(len(all_clues_10per))\n",
    "    print(all_clues_10per[0])\n",
    "\n",
    "    # logic the same as scrape_parse.guardian_load.make_disjoint_split()\n",
    "    def make_dataset(disj: bool):\n",
    "        # make json\n",
    "        json_all = []\n",
    "        for c in all_clues_10per:\n",
    "            c.soln = c.soln.lower()\n",
    "            json_dict = dict(defn=c.clue,\n",
    "                             target=c.soln)\n",
    "            json_all.append(json_dict)\n",
    "\n",
    "        train, val = [], []     # list of json dicts\n",
    "        rng.seed(seed)\n",
    "        if disj:\n",
    "            for c in json_all:\n",
    "                idx = hash(c['target'][:2]) % 10\n",
    "                if idx == 0:\n",
    "                    val.append(c)   # val ~ 20k\n",
    "                else:\n",
    "                    train.append(c)\n",
    "            rng.shuffle(train)\n",
    "            rng.shuffle(val)\n",
    "        else:   # not disj\n",
    "            train, val = train_test_split(json_all, test_size=.1, random_state=seed)\n",
    "\n",
    "        return train, val\n",
    "\n",
    "    write_json_tuple(list(make_dataset(disj=False)),    # train, val\n",
    "                     comment=\"\",\n",
    "                     export_dir=k_descramble_rand)\n",
    "\n",
    "    write_json_tuple(list(make_dataset(disj=True)),     # train, val\n",
    "                     comment=\"\",\n",
    "                     export_dir=k_descramble_disj)\n",
    "\n",
    "make_descramble_json()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "todo: how to run with the descramble setup tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in the collater, produce <scramble> | defn -> target\n",
    "\n",
    "## 6.4 Wordplay systematic learning\n",
    "- identify clues with anagram of first name\n",
    "- two sets: scramble / substitute\n",
    "- run / evaluate\n",
    "- average character level overlap\n",
    "\n",
    "## 6.5  efrat comparison\n",
    "- replication - hyperparams / model calls\n",
    "- create word initial version\n",
    "- how to run curricular"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}