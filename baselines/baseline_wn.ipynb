{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'Heuristic wordnet baseline'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Heuristic wordnet baseline\"\"\"\n",
    "###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "k_json_folder = '../puzzles/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from decrypt.scrape_parse import (\n",
    "    load_guardian_splits,\n",
    "    load_guardian_splits_disjoint_hash\n",
    ")\n",
    "\n",
    "import random\n",
    "from typing import *\n",
    "\n",
    "import jellyfish\n",
    "\n",
    "from multiset import Multiset\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from decrypt.common.puzzle_clue import GuardianClue\n",
    "from decrypt.common.util_wordnet import all_inflect\n",
    "from decrypt.common import validation_tools as vt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Wordnet functions to produce reverse dictionary sets\n",
    "\n",
    "def normalize(lemma):\n",
    "    \"\"\"Wordnet returns words with underscores and hyphens. We replace them with spaces. This possibly does not work well with lemminflect.\"\"\"\n",
    "    return lemma.replace(\"_\",\" \").replace(\"-\",\" \")\n",
    "\n",
    "def get_syns(w: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Get all synonyms of w\n",
    "    \"\"\"\n",
    "    ret = set()\n",
    "    for ss in wn.synsets(w):\n",
    "        for l in ss.lemma_names():\n",
    "            ret.add(normalize(l))\n",
    "    return ret\n",
    "\n",
    "def get_syns_hypo1(w: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Get all synonyms and hyponyms to depth 1\n",
    "    \"\"\"\n",
    "    ret = set()\n",
    "    for ss in wn.synsets(w):\n",
    "        for l in ss.lemma_names():\n",
    "            ret.add(normalize(l))\n",
    "        for rel_ss in ss.hyponyms():\n",
    "            for l in rel_ss.lemma_names():\n",
    "                ret.add(normalize(l))\n",
    "    return ret\n",
    "\n",
    "def get_syns_hypo_all(w: str, include_hyper=False, depth=3) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Get all synonyms; hyponyms to depth, depth; and hypernyms to depth, depth,\n",
    "    if include_hyper is True\n",
    "\n",
    "    :param w: word to lookup\n",
    "    :param include_hyper: whether to do hypernym lookup\n",
    "    :param depth: how far to go in hyponym / hypernym traversal\n",
    "    \"\"\"\n",
    "    ret = set()\n",
    "    for ss in wn.synsets(w):\n",
    "        for l in ss.lemma_names():\n",
    "            ret.add(normalize(l))\n",
    "        if include_hyper:\n",
    "            for rel_ss in ss.closure(lambda s: s.hypernyms(), depth=depth):\n",
    "                for l in rel_ss.lemma_names():\n",
    "                    ret.add(normalize(l))\n",
    "        for rel_ss in ss.closure(lambda s: s.hyponyms(), depth=depth):\n",
    "            for l in rel_ss.lemma_names():\n",
    "                ret.add(normalize(l))\n",
    "    return ret\n",
    "\n",
    "def get_first_and_last_word(c: GuardianClue):\n",
    "    clue_words = c.clue.split(\" \")\n",
    "    return clue_words[0], clue_words[-1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def pct_sim(str1, str2):\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    lev = jellyfish.levenshtein_distance(str1, str2)\n",
    "    return 1.0 - lev/max_len\n",
    "\n",
    "def eval_wn(val_set: List[GuardianClue],\n",
    "            fcn: Callable,\n",
    "            do_fuzzy: bool,\n",
    "            do_rank: bool = False,\n",
    "            **fcn_kwargs):\n",
    "    \"\"\"\n",
    "    :param val_set:\n",
    "    :param fcn:\n",
    "    :param do_fuzzy:\n",
    "    :param fcn_kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rng = random.Random()\n",
    "    rng.seed(42)\n",
    "\n",
    "    model_outputs = []\n",
    "    for val_gc in tqdm(val_set):\n",
    "        all_possible = set()\n",
    "\n",
    "        # add the direct synonyms\n",
    "        for w in get_first_and_last_word(val_gc):\n",
    "            all_possible.update(list(fcn(w.lower(), **fcn_kwargs)))\n",
    "\n",
    "        # potentially add lemmas\n",
    "        if do_fuzzy:\n",
    "            orig = all_possible.copy()\n",
    "            for w in orig:\n",
    "                all_possible.update(all_inflect(w, None))\n",
    "\n",
    "        _, filtered = vt.filter_to_len(val_gc.soln_with_spaces, all_possible)\n",
    "        filtered_final = [x[0] for x in filtered]   # go back to with spaces\n",
    "\n",
    "        # jellyfish score\n",
    "        # # if do_rank:\n",
    "        # #     list_with_rank = []\n",
    "        # #     for out in filtered_final:\n",
    "        # #         score = pct_sim(out, val_gc.clue)\n",
    "        # #         list_with_rank.append((out, score))\n",
    "        # #     # sort\n",
    "        # #     list_sorted = sorted(list_with_rank, key=lambda x: x[1], reverse=True)\n",
    "        # #     # take the word not the score\n",
    "        #     filtered_final = [x[0] for x in list_sorted]\n",
    "\n",
    "        # simple character overlap\n",
    "        if do_rank:\n",
    "            list_with_rank = []\n",
    "            mset = Multiset(val_gc.clue)\n",
    "            for out in filtered_final:\n",
    "                score = len(mset.intersection(Multiset(out)))\n",
    "                list_with_rank.append((out, score))\n",
    "            # sort\n",
    "            list_sorted = sorted(list_with_rank, key=lambda x: x[1], reverse=True)\n",
    "            # take the word not the score\n",
    "            filtered_final = [x[0] for x in list_sorted]\n",
    "        else:\n",
    "            rng.shuffle(filtered_final)\n",
    "\n",
    "        mp = vt.ModelPrediction(\n",
    "            idx=val_gc.idx,\n",
    "            input=val_gc.clue_with_lengths(),\n",
    "            target=val_gc.soln_with_spaces,\n",
    "            greedy=\"\",\n",
    "            sampled=filtered_final)\n",
    "\n",
    "        mp.model_eval = vt.eval(mp)\n",
    "        model_outputs.append(mp)\n",
    "\n",
    "    return model_outputs\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5518/5518 [00:10<00:00, 506.11it/s]\n",
      "100%|██████████| 143991/143991 [00:00<00:00, 728162.12it/s]\n",
      "  0%|          | 0/55783 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"length punct: '\", 1),\n",
      " ('invalid: clue group', 7687),\n",
      " ('invalid: invalid start char (most are continuation clues)', 607),\n",
      " ('invalid: number in clue (commonly references another clue)', 7066),\n",
      " ('invalid: regexp', 75),\n",
      " ('invalid: soln length does not match specified lens (multi box soln)', 56),\n",
      " ('invalid: unrecognized char in clue (e.g. html)', 85),\n",
      " ('invalid: zero-len clue text after regexp', 15),\n",
      " ('length punct: ,', 24644),\n",
      " ('length punct: -', 4148),\n",
      " ('length punct: .', 8),\n",
      " ('length punct: /', 1),\n",
      " ('stat: parsed_puzzle', 5518),\n",
      " ('stat: total_clues', 143991),\n",
      " (1, 119956),\n",
      " (2, 20272),\n",
      " (3, 2957),\n",
      " (4, 686),\n",
      " (5, 112),\n",
      " (6, 8)]\n",
      "Total clues: len(puzz_list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55783/55783 [00:02<00:00, 20214.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 1611 exact dupes\n",
      "142380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28476/28476 [00:37<00:00, 761.80it/s] \n",
      "  0%|          | 92/28476 [00:00<00:31, 913.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('agg_filter_len_pre_truncate', 10.313948588284871),\n",
      " ('agg_filtered_few', 0.6430327293159152),\n",
      " ('agg_generate_few', 0.6430327293159152),\n",
      " ('agg_generate_none', 0.12824834948728753),\n",
      " ('agg_in_filtered', 0.13119820199466217),\n",
      " ('agg_in_sample', 0.1078803202697008),\n",
      " ('agg_sample_len', 5.7325467059980335),\n",
      " ('agg_sample_len_correct', 1.0),\n",
      " ('agg_sample_len_pre_truncate', 10.313948588284871),\n",
      " ('agg_sample_wordct_correct', 0.7355305072286205),\n",
      " ('agg_top_10_after_filter', 0.1078803202697008),\n",
      " ('agg_top_match', 0.028761061946902654),\n",
      " ('agg_top_match_len_correct', 0.8717516505127124),\n",
      " ('agg_top_match_none', 0.12824834948728753),\n",
      " ('agg_top_match_wordct_correct', 0.554291333052395),\n",
      " ('agg_top_sample_result_len_correct', 0.8717516505127124),\n",
      " ('agg_top_sample_result_wordct_correct', 0.554291333052395),\n",
      " ('filter_len_pre_truncate', 293700),\n",
      " ('filtered_few', 18311),\n",
      " ('generate_few', 18311),\n",
      " ('generate_none', 3652),\n",
      " ('in_filtered', 3736),\n",
      " ('in_sample', 3072),\n",
      " ('sample_len', 163240),\n",
      " ('sample_len_correct', 163240),\n",
      " ('sample_len_pre_truncate', 293700),\n",
      " ('sample_wordct_correct', 120068),\n",
      " ('top_10_after_filter', 3072),\n",
      " ('top_match', 819),\n",
      " ('top_match_len_correct', 24824),\n",
      " ('top_match_none', 3652),\n",
      " ('top_match_wordct_correct', 15784),\n",
      " ('top_sample_result_len_correct', 24824),\n",
      " ('top_sample_result_wordct_correct', 15784),\n",
      " ('total', 28476)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28476/28476 [00:40<00:00, 710.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('agg_filter_len_pre_truncate', 10.353912066301447),\n",
      " ('agg_filtered_few', 0.6434541368169687),\n",
      " ('agg_generate_few', 0.6434541368169687),\n",
      " ('agg_generate_none', 0.12891557803062229),\n",
      " ('agg_in_filtered', 0.13260289366484057),\n",
      " ('agg_in_sample', 0.10756426464391067),\n",
      " ('agg_sample_len', 5.748911363955612),\n",
      " ('agg_sample_len_correct', 1.0),\n",
      " ('agg_sample_len_pre_truncate', 10.353912066301447),\n",
      " ('agg_sample_wordct_correct', 0.7319035343848118),\n",
      " ('agg_top_10_after_filter', 0.10756426464391067),\n",
      " ('agg_top_match', 0.025705857564264644),\n",
      " ('agg_top_match_len_correct', 0.8710844219693777),\n",
      " ('agg_top_match_none', 0.12891557803062229),\n",
      " ('agg_top_match_wordct_correct', 0.5543264503441495),\n",
      " ('agg_top_sample_result_len_correct', 0.8710844219693777),\n",
      " ('agg_top_sample_result_wordct_correct', 0.5543264503441495),\n",
      " ('filter_len_pre_truncate', 294838),\n",
      " ('filtered_few', 18323),\n",
      " ('generate_few', 18323),\n",
      " ('generate_none', 3671),\n",
      " ('in_filtered', 3776),\n",
      " ('in_sample', 3063),\n",
      " ('sample_len', 163706),\n",
      " ('sample_len_correct', 163706),\n",
      " ('sample_len_pre_truncate', 294838),\n",
      " ('sample_wordct_correct', 119817),\n",
      " ('top_10_after_filter', 3063),\n",
      " ('top_match', 732),\n",
      " ('top_match_len_correct', 24805),\n",
      " ('top_match_none', 3671),\n",
      " ('top_match_wordct_correct', 15785),\n",
      " ('top_sample_result_len_correct', 24805),\n",
      " ('top_sample_result_wordct_correct', 15785),\n",
      " ('total', 28476)]\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# this is the primary baseline\n",
    "######################\n",
    "\n",
    "# naive set\n",
    "def run_primary_wn_naive():\n",
    "    _, _, (_, val_orig, test_orig) = load_guardian_splits(k_json_folder)\n",
    "    out1 = eval_wn(val_orig, fcn=get_syns_hypo1, do_fuzzy=False, do_rank=True) # 1711\n",
    "    vt.all_aggregate(out1, label='syns,hypo1; no fuzzy, ranked by char overlap')\n",
    "\n",
    "    out2 = eval_wn(test_orig, fcn=get_syns_hypo1, do_fuzzy=False, do_rank=True) # 1711\n",
    "    vt.all_aggregate(out2, label='syns,hypo1; no fuzzy, ranked by char overlap')\n",
    "\n",
    "run_primary_wn_naive()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# disjoint set\n",
    "# soln_to_clue_map, all_clues, (train, val, test) = load_guardian_splits_disjoint_hash(DataDirs.Guardian.json_folder)\n",
    "\n",
    "# naive set\n",
    "def run_primary_wn_disj2():\n",
    "    _, _, (_, val_orig, test_orig) = load_guardian_splits_disjoint_hash(k_json_folder)\n",
    "    out1 = eval_wn(val_orig, fcn=get_syns_hypo1, do_fuzzy=False, do_rank=True) # 1711\n",
    "    vt.all_aggregate(out1, label='syns,hypo1; no fuzzy, ranked by char overlap')\n",
    "\n",
    "    out2 = eval_wn(test_orig, fcn=get_syns_hypo1, do_fuzzy=False, do_rank=True) # 1711\n",
    "    vt.all_aggregate(out2, label='syns,hypo1; no fuzzy, ranked by char overlap')\n",
    "\n",
    "run_primary_wn_disj2()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}